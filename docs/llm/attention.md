---
date: 2025-04-02
categories:
  - LLM
search:
  boost: 2
hide:
  - footer
---

## MHA

MHA（Multi-Head Attention），也就是多头注意力。

[Attention Is All You Need](https://arxiv.org/abs/1706.03762)

## MQA

MQA，即“Multi-Query Attention”，是减少KV Cache的一次非常朴素的尝试。

[Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/abs/1911.02150)

> https://kexue.fm/archives/4765

## GQA

GQA（Grouped-Query Attention）

[GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/abs/2305.13245)

## MLA

MLA（Multi-head Latent Attention）


[DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434)
